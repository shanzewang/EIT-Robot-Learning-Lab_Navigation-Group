# -*- coding: utf-8 -*-
"""
Soft Actor-Criticå¼ºåŒ–å­¦ä¹ ä¸»è®­ç»ƒæ–‡ä»¶ (PyTorchç‰ˆæœ¬)
=======================================================

æœ¬æ–‡ä»¶å®ç°å®Œæ•´çš„SACç®—æ³•è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. SACç®—æ³•æ ¸å¿ƒå®ç°
2. ç»éªŒå›æ”¾ç¼“å†²åŒºç®¡ç†
3. è¯¾ç¨‹å­¦ä¹ å’Œéš¾åº¦è‡ªé€‚åº”
4. å¤šå®éªŒå¹¶è¡Œå’Œæ€§èƒ½è¯„ä¼°
5. å®Œæ•´çš„æ—¥å¿—ç®¡ç†å’Œå¯è§†åŒ–ç³»ç»Ÿ

è°ƒç”¨å…³ç³»ï¼š
- ä½¿ç”¨core_config_pytorchä¸­çš„ç¥ç»ç½‘ç»œæ¶æ„
- ä¸stage_obsç¯å¢ƒè¿›è¡Œäº¤äº’
- ç®¡ç†ROS+Stageä»¿çœŸç¯å¢ƒ
- ä½¿ç”¨TrainingLoggerè¿›è¡Œæ—¥å¿—ç®¡ç†å’Œå¯è§†åŒ–
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter

import gym
import time
import torchcore_true as core  # ä½¿ç”¨PyTorchç‰ˆæœ¬çš„ç½‘ç»œæ–‡ä»¶
from stage_obs_comment import StageWorld  # ä½¿ç”¨æ–°çš„ç¯å¢ƒæ–‡ä»¶
from training_logger import TrainingLogger  # å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ
import random
import rospy
import os
import signal
import subprocess
import sys
from collections import deque
import scipy.stats as stats
from scipy.stats import rankdata
from scipy.stats import truncnorm


class ReplayBuffer:
    """
    ç»éªŒå›æ”¾ç¼“å†²åŒº
    
    ä½œç”¨ï¼šå­˜å‚¨å’Œé‡‡æ ·æ™ºèƒ½ä½“çš„å†å²ç»éªŒï¼Œæ”¯æŒç¦»çº¿ç­–ç•¥å­¦ä¹ 
    ç‰¹ç‚¹ï¼šFIFOç»“æ„ï¼Œæ”¯æŒåŠ¨æ€é‡‡æ ·ç­–ç•¥
    """

    def __init__(self, obs_dim, act_dim, size):
        """
        åˆå§‹åŒ–ç»éªŒç¼“å†²åŒº
        
        å‚æ•°ï¼š
            obs_dim - è§‚æµ‹ç»´åº¦(548)
            act_dim - åŠ¨ä½œç»´åº¦(2)
            size - ç¼“å†²åŒºå®¹é‡(2M)
        """
        # é¢„åˆ†é…å†…å­˜å­˜å‚¨ç»éªŒæ•°æ®
        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)  # å½“å‰çŠ¶æ€
        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)  # ä¸‹ä¸€çŠ¶æ€
        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)  # åŠ¨ä½œ
        self.rews_buf = np.zeros(size, dtype=np.float32)             # å¥–åŠ±
        self.done_buf = np.zeros(size, dtype=np.float32)             # ç»ˆæ­¢æ ‡å¿—
        
        # ç¼“å†²åŒºç®¡ç†å˜é‡
        self.ptr = 0        # å½“å‰å†™å…¥ä½ç½®
        self.size = 0       # å½“å‰æ•°æ®é‡
        self.max_size = size # æœ€å¤§å®¹é‡

    def store(self, obs, act, rew, next_obs, done):
        """
        å­˜å‚¨ä¸€æ¡ç»éªŒ
        
        å‚æ•°ï¼š
            obs - å½“å‰çŠ¶æ€
            act - æ‰§è¡ŒåŠ¨ä½œ
            rew - è·å¾—å¥–åŠ±
            next_obs - ä¸‹ä¸€çŠ¶æ€
            done - æ˜¯å¦ç»ˆæ­¢
        """
        self.obs1_buf[self.ptr] = obs
        self.obs2_buf[self.ptr] = next_obs
        self.acts_buf[self.ptr] = act
        self.rews_buf[self.ptr] = rew
        self.done_buf[self.ptr] = done
        
        self.ptr = (self.ptr + 1) % self.max_size  # å¾ªç¯è¦†ç›–
        self.size = min(self.size + 1, self.max_size)

    def sample_batch(self, batch_size=128, start=0):
        """
        é‡‡æ ·è®­ç»ƒæ‰¹æ¬¡
        
        å‚æ•°ï¼š
            batch_size - æ‰¹æ¬¡å¤§å°
            start - é‡‡æ ·èµ·å§‹ä½ç½®(ç”¨äºåå‘æ–°ç»éªŒçš„é‡‡æ ·ç­–ç•¥)
        
        è¿”å›ï¼šåŒ…å«obs1, obs2, acts, rews, doneçš„å­—å…¸
        """
        idxs = np.random.randint(int(start), self.size, size=batch_size)
        return dict(
            obs1=self.obs1_buf[idxs],
            obs2=self.obs2_buf[idxs],
            acts=self.acts_buf[idxs],
            rews=self.rews_buf[idxs],
            done=self.done_buf[idxs],
        )


class StageWorldLogger(StageWorld):
    """æ‰©å±•StageWorldç±»ï¼Œæ·»åŠ æ—¥å¿—åŠŸèƒ½"""
    
    def __init__(self, beam_num, logger=None):
        super().__init__(beam_num)
        self.logger = logger
        self.step_context = "training"  # å½“å‰stepè°ƒç”¨çš„ä¸Šä¸‹æ–‡
    
    def set_step_context(self, context):
        """è®¾ç½®stepè°ƒç”¨çš„ä¸Šä¸‹æ–‡"""
        self.step_context = context
    
    def step(self):
        """é‡å†™stepå‡½æ•°ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ—¥å¿—"""
        # è°ƒç”¨åŸå§‹stepå‡½æ•°
        state, reward, terminate, reset, distance, robot_pose = super().step()
        
        # æ ¹æ®ä¸Šä¸‹æ–‡å†³å®šæ˜¯å¦è®°å½•ç¢°æ’
        if terminate and reset == 0:  # ç¢°æ’ç»ˆæ­¢
            if self.logger and self.step_context == "initialization":
                self.logger.log_crash("initialization")
            # è®­ç»ƒæ—¶çš„ç¢°æ’åœ¨episode_endä¸­ç»Ÿä¸€å¤„ç†ï¼Œè¿™é‡Œä¸é‡å¤è¾“å‡º
        
        return state, reward, terminate, reset, distance, robot_pose


def sac(
    actor_critic=core.MLPActorCritic,
    seed=5,
    steps_per_epoch=5000,
    epochs=10000,
    replay_size=int(2e6),
    gamma=0.99,
    polyak=0.995,
    lr1=1e-4,
    lr2=1e-4,
    alpha=0.01,
    batch_size=100,
    start_epoch=100,
    max_ep_len=400,
    MAX_EPISODE=10000,
    device='cpu',
):
    """
    Soft Actor-Criticç®—æ³•å®ç° (PyTorchç‰ˆæœ¬)
    
    SACæ ¸å¿ƒæ€æƒ³ï¼šæœ€å¤§åŒ–æœŸæœ›å¥–åŠ±å’Œç­–ç•¥ç†µçš„åŠ æƒå’Œ
    ç›®æ ‡å‡½æ•°ï¼šJ(Ï€) = E[r(s,a) + Î±Â·H(Ï€(Â·|s))]
    
    å‚æ•°è¯´æ˜ï¼š
        actor_critic - ç¥ç»ç½‘ç»œæ¶æ„ç±»
        seed - éšæœºç§å­
        steps_per_epoch - æ¯epochæ­¥æ•°
        epochs - æ€»epochæ•°
        replay_size - ç»éªŒå›æ”¾å®¹é‡
        gamma - æŠ˜æ‰£å› å­
        polyak - ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç³»æ•°
        lr1/lr2 - ä»·å€¼/ç­–ç•¥ç½‘ç»œå­¦ä¹ ç‡
        alpha - ç†µæ­£åˆ™åŒ–ç³»æ•°
        batch_size - è®­ç»ƒæ‰¹å¤§å°
        start_epoch - å¼€å§‹ä½¿ç”¨å­¦ä¹ ç­–ç•¥çš„epoch
        max_ep_len - æœ€å¤§episodeé•¿åº¦
        MAX_EPISODE - æœ€å¤§episodeæ•°
        device - è®¡ç®—è®¾å¤‡('cpu'æˆ–'cuda')
    """
    
    # ============= åŸºç¡€å‚æ•°è®¾ç½® =============
    sac_id = 1             # ç®—æ³•æ ‡è¯†
    obs_dim = 540 + 8      # è§‚æµ‹ç»´åº¦ï¼šæ¿€å…‰é›·è¾¾540 + ç›®æ ‡2 + é€Ÿåº¦2 + åŠ¨åŠ›å­¦å‚æ•°4 = 548
    act_dim = 2            # åŠ¨ä½œç»´åº¦ï¼šçº¿é€Ÿåº¦+è§’é€Ÿåº¦
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(device if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # ============= æ„å»ºç¥ç»ç½‘ç»œ =============
    # ä¸»ç½‘ç»œï¼šç”¨äºè®­ç»ƒ
    main_model = actor_critic(obs_dim, act_dim).to(device)
    
    # ç›®æ ‡ç½‘ç»œï¼šç”¨äºè®¡ç®—ç›®æ ‡Qå€¼ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
    target_model = actor_critic(obs_dim, act_dim).to(device)
    
    # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œå‚æ•°ä¸ä¸»ç½‘ç»œä¸€è‡´
    target_model.load_state_dict(main_model.state_dict())
    
    # å†»ç»“ç›®æ ‡ç½‘ç»œå‚æ•°ï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
    for param in target_model.parameters():
        param.requires_grad = False

    # ç»éªŒå›æ”¾ç¼“å†²åŒº
    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)

    # ç½‘ç»œå‚æ•°ç»Ÿè®¡
    total_params = core.count_vars(main_model)
    policy_params = core.count_vars(main_model.policy)
    q1_params = core.count_vars(main_model.q1)
    q2_params = core.count_vars(main_model.q2)
    print(f'\nç½‘ç»œå‚æ•°æ•°é‡: total: {total_params}, policy: {policy_params}, '
          f'q1: {q1_params}, q2: {q2_params}\n')

    # ============= ä¼˜åŒ–å™¨è®¾ç½® =============
    # ç­–ç•¥ç½‘ç»œä¼˜åŒ–å™¨
    pi_optimizer = optim.Adam(main_model.policy.parameters(), lr=lr2)
    
    # ä»·å€¼ç½‘ç»œä¼˜åŒ–å™¨
    q_optimizer = optim.Adam(
        list(main_model.q1.parameters()) + 
        list(main_model.q2.parameters()) + 
        list(main_model.cnn_dense.parameters()), 
        lr=lr1
    )
    
    # L2æ­£åˆ™åŒ–
    l2_reg = 0.001

    def get_action(o, deterministic=False):
        """
        è·å–åŠ¨ä½œ
        
        å‚æ•°ï¼š
            o - çŠ¶æ€è§‚æµ‹
            deterministic - æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥(æµ‹è¯•æ—¶ä¸ºTrue)
        
        è¿”å›ï¼šåŠ¨ä½œå‘é‡
        """
        with torch.no_grad():
            o_tensor = torch.FloatTensor(o.reshape(1, -1)).to(device)
            
            # è°ƒç”¨å®Œæ•´ç½‘ç»œè·å–æ‰€æœ‰è¾“å‡º
            # main_model(o_tensor) åªä¼ å…¥çŠ¶æ€ï¼Œè¿”å›5ä¸ªå€¼ï¼šmu, pi, logp_pi, q1, q2
            mu, pi, logp_pi, _, _ = main_model(o_tensor)
            
            if deterministic:
                # ç¡®å®šæ€§ç­–ç•¥ï¼šä½¿ç”¨tanh(mu)
                action = torch.tanh(mu)
                return action.cpu().numpy()[0]
            else:
                # éšæœºç­–ç•¥ï¼šä½¿ç”¨é‡‡æ ·çš„åŠ¨ä½œpi
                return pi.cpu().numpy()[0]

    def update_networks(batch):
        """
        æ›´æ–°ç¥ç»ç½‘ç»œ
        
        å‚æ•°ï¼š
            batch - è®­ç»ƒæ‰¹æ¬¡æ•°æ®
        
        è¿”å›ï¼š
            pi_loss, q_loss - ç­–ç•¥æŸå¤±å’ŒQç½‘ç»œæŸå¤±
        """
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        obs1 = torch.FloatTensor(batch['obs1']).to(device)
        obs2 = torch.FloatTensor(batch['obs2']).to(device)
        acts = torch.FloatTensor(batch['acts']).to(device)
        rews = torch.FloatTensor(batch['rews']).to(device)
        done = torch.FloatTensor(batch['done']).to(device)

        # ============= æ›´æ–°Qç½‘ç»œ =============
        with torch.no_grad():
            # ç›®æ ‡ç½‘ç»œè®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„Qå€¼
            # target_model(obs2) åªä¼ å…¥çŠ¶æ€ï¼Œè¿”å›5ä¸ªå€¼ï¼šmu, pi, logp_pi, q1_pi, q2_pi
            _, pi_next, logp_pi_next, q1_pi_targ, q2_pi_targ = target_model(obs2)
            
            # åŒQç½‘ç»œå–æœ€å°å€¼ï¼Œå‡å°‘è¿‡ä¼°è®¡
            min_q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)
            
            # è®¡ç®—ç›®æ ‡å€¼ï¼šr + Î³(1-done)[Q_min - Î±*log_Ï€]
            backup = rews + gamma * (1 - done) * (min_q_pi_targ - alpha * logp_pi_next)

        # å½“å‰Qå€¼
        # main_model(obs1, acts) ä¼ å…¥çŠ¶æ€å’ŒåŠ¨ä½œï¼Œè¿”å›7ä¸ªå€¼
        mu, pi, logp_pi, q1, q2, q1_pi, q2_pi = main_model(obs1, acts)
        
        # QæŸå¤±
        q1_loss = F.mse_loss(q1, backup)
        q2_loss = F.mse_loss(q2, backup)
        q_loss = q1_loss + q2_loss

        # æ›´æ–°Qç½‘ç»œ
        q_optimizer.zero_grad()
        q_loss.backward()
        q_optimizer.step()

        # ============= æ›´æ–°ç­–ç•¥ç½‘ç»œ =============
        # å†»ç»“Qç½‘ç»œå‚æ•°ï¼Œåªæ›´æ–°ç­–ç•¥ç½‘ç»œ
        for param in main_model.q1.parameters():
            param.requires_grad = False
        for param in main_model.q2.parameters():
            param.requires_grad = False
        for param in main_model.cnn_dense.parameters():
            param.requires_grad = False

        # é‡æ–°è®¡ç®—ç­–ç•¥ç½‘ç»œè¾“å‡ºï¼ˆåªä¼ å…¥çŠ¶æ€ï¼Œè®©ç½‘ç»œé‡æ–°é‡‡æ ·åŠ¨ä½œï¼‰
        # main_model(obs1) åªä¼ å…¥çŠ¶æ€ï¼Œè¿”å›5ä¸ªå€¼ï¼šmu, pi, logp_pi, q1_pi, q2_pi
        mu, pi, logp_pi, q1_pi, q2_pi = main_model(obs1)
        
        # ç­–ç•¥æŸå¤±ï¼šæœ€å¤§åŒ– Q(s,Ï€(s)) - Î±*log_Ï€(s)
        min_q_pi = torch.min(q1_pi, q2_pi)
        
        # L2æ­£åˆ™åŒ–
        l2_penalty = sum(param.pow(2.0).sum() for param in main_model.policy.parameters())
        
        pi_loss = torch.mean(alpha * logp_pi - min_q_pi) + l2_reg * l2_penalty

        # æ›´æ–°ç­–ç•¥ç½‘ç»œ
        pi_optimizer.zero_grad()
        pi_loss.backward()
        pi_optimizer.step()

        # è§£å†»Qç½‘ç»œå‚æ•°
        for param in main_model.q1.parameters():
            param.requires_grad = True
        for param in main_model.q2.parameters():
            param.requires_grad = True
        for param in main_model.cnn_dense.parameters():
            param.requires_grad = True

        # ============= è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ =============
        with torch.no_grad():
            for param_main, param_target in zip(main_model.parameters(), target_model.parameters()):
                param_target.data.mul_(polyak)
                param_target.data.add_((1 - polyak) * param_main.data)

        return pi_loss.item(), q_loss.item()

    # ============= TensorBoardè®¾ç½® =============
    log_dir = f'./logssac{sac_id}'
    summary_writer = SummaryWriter(log_dir)

    # ============= è®­ç»ƒåˆå§‹åŒ– =============
    episode = 0              # episodeè®¡æ•°
    T = 0                   # æ€»è®­ç»ƒæ­¥æ•°
    epi_thr = 0

    # å®éªŒæ•°æ®å­˜å‚¨
    suc_record_all = np.zeros((5, 150, 9))
    suc_record_all_new = np.zeros((5, 150, 9))
    
    # åŠ è½½æµ‹è¯•ç»“æœå­˜å‚¨æ•°ç»„ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°çš„ï¼‰
    try:
        test_result_plot = np.load('test_result_plot1.npy')
    except FileNotFoundError:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„æ•°ç»„ï¼š[å®éªŒæ•°, å°ºå¯¸ç»„, æµ‹è¯•æ¬¡æ•°, æµ‹è¯•è½®æ¬¡, æŒ‡æ ‡æ•°]
        test_result_plot = np.zeros((5, 5, 50, 101, 5))
        print("åˆ›å»ºæ–°çš„æµ‹è¯•ç»“æœå­˜å‚¨æ•°ç»„")
    
    env_record = np.zeros((5, 4))
    train_result = np.zeros((5, 12000))
    test_time = 0

    # ============= å¤šå®éªŒå¾ªç¯ =============
    for hyper_exp in range(1, 5):  # è¿è¡Œ4ä¸ªç‹¬ç«‹å®éªŒ
        print(f"\n========== å®éªŒ {hyper_exp} å¼€å§‹ ==========")
        
        # ============= åˆ›å»ºæ—¥å¿—ç®¡ç†å™¨ =============
        logger = TrainingLogger(experiment_id=hyper_exp)
        
        # ============= åˆ›å»ºå¸¦æ—¥å¿—åŠŸèƒ½çš„ç¯å¢ƒ =============
        env = StageWorldLogger(540, logger=logger)
        
        # ============= ROSé¢‘ç‡æ§åˆ¶ï¼ˆåœ¨ç¯å¢ƒåˆ›å»ºåï¼‰ =============
        rate = rospy.Rate(10)   # ROSé¢‘ç‡æ§åˆ¶
        
        # å®éªŒåˆå§‹åŒ–
        goal_reach = 0
        past_env = [0]
        current_env = [0]
        
        # è¯¾ç¨‹å­¦ä¹ å˜é‡
        suc_record = np.zeros((5, 50))    # æˆåŠŸè®°å½•ï¼š5ä¸ªæœºå™¨äººå°ºå¯¸ç»„Ã—50æ¬¡è®°å½•
        suc_record1 = np.zeros((5, 50))   # æ— ç¢°æ’æˆåŠŸè®°å½•
        suc_record2 = np.zeros((5, 50))   # è¶…æ—¶è®°å½•
        suc_pointer = np.zeros(5)         # è®°å½•æŒ‡é’ˆ
        mean_rate = np.zeros(5)           # å¹³å‡æˆåŠŸç‡
        env_list = np.zeros(5)            # å„ç»„å½“å‰ç¯å¢ƒéš¾åº¦(0-6)
        p = np.zeros(9)
        p[0] = 1.0                        # åˆå§‹åªä½¿ç”¨ç¯å¢ƒ0
        mean_rate[0] = 0.0
        best_value = 0
        
        # è®¾ç½®éšæœºç§å­
        seed = hyper_exp
        torch.manual_seed(seed)
        np.random.seed(seed)
        
        # é‡æ–°åˆå§‹åŒ–ç½‘ç»œ
        main_model = actor_critic(obs_dim, act_dim).to(device)
        target_model = actor_critic(obs_dim, act_dim).to(device)
        target_model.load_state_dict(main_model.state_dict())
        
        for param in target_model.parameters():
            param.requires_grad = False
            
        # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨
        pi_optimizer = optim.Adam(main_model.policy.parameters(), lr=lr2)
        q_optimizer = optim.Adam(
            list(main_model.q1.parameters()) + 
            list(main_model.q2.parameters()) + 
            list(main_model.cnn_dense.parameters()), 
            lr=lr1
        )
        
        replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)
        
        # é‡ç½®å®éªŒå˜é‡
        episode = 0
        T = 0
        epi_thr = 0
        goal_reach = 0
        test_time = 0
        new_env = 0          # å½“å‰è§£é”çš„æœ€é«˜ç¯å¢ƒç­‰çº§
        succ_rate_test = 0
        b_test = True
        length_index = 0

        # ============= ä¸»è®­ç»ƒå¾ªç¯ =============
        while test_time < 101:  # è¿›è¡Œ101æ¬¡æ€§èƒ½æµ‹è¯•
            
            # ç¯å¢ƒéš¾åº¦é€‰æ‹©
            if new_env == 0:
                env_no = 0  # åˆæœŸåªä½¿ç”¨æœ€ç®€å•ç¯å¢ƒ
            else:
                env_no = new_env
            
            # éšæœºæœºå™¨äººå°ºå¯¸(åŸŸéšæœºåŒ–æé«˜æ³›åŒ–æ€§)
            length_index = np.random.choice(range(5))
            length1 = np.random.uniform(0.075, 0.6)    # å‰æ–¹é•¿åº¦
            length2 = np.random.uniform(0.075, 0.6)    # åæ–¹é•¿åº¦
            width = np.random.uniform(0.075, (length2 + length1) / 2.0)  # å®½åº¦
            
            env_no = int(env_list[length_index])

            # æœºå™¨äººå°ºå¯¸çº¦æŸæ£€æŸ¥
            if length_index == 0:
                while length1 + length2 + width * 2 >= 0.8:
                    length1 = np.random.uniform(0.075, 0.6)
                    length2 = np.random.uniform(0.075, 0.6)
                    width = np.random.uniform(0.075, (length2 + length1) / 2.0)
            else:
                while (length1 + length2 + width * 2 < (length_index + 1.0) * 0.4 or 
                       length1 + length2 + width * 2 >= (length_index + 2.0) * 0.4):
                    length1 = np.random.uniform(0.075, 0.6)
                    length2 = np.random.uniform(0.075, 0.6)
                    width = np.random.uniform(0.075, (length2 + length1) / 2.0)

            # ============= è®¾ç½®è®­ç»ƒé˜¶æ®µ =============
            logger.set_phase("TRAINING", 
                            env=int(env_list[length_index]), 
                            robot_size=[length1, length2, width])

            # Episodeåˆå§‹åŒ–
            T_step = 0
            goal_reach = 0

            # ============= è®¾ç½®stepä¸Šä¸‹æ–‡ä¸ºåˆå§‹åŒ– =============
            env.set_step_context("initialization")

            # é‡ç½®ç¯å¢ƒ
            if goal_reach == 1 and b_test:
                robot_size = env.Reset(env_no)
            else:
                robot_size = env.ResetWorld(env_no, length1, length2, width)
                b_test = True

            # ç”Ÿæˆç›®æ ‡ç‚¹(åŸºäºæˆåŠŸç‡çš„è‡ªé€‚åº”ç›®æ ‡ç”Ÿæˆ)
            env.GenerateTargetPoint(mean_rate[length_index])
            o, r, d, goal_reach, r2gd, robot_pose = env.step()
            rate.sleep()

            # ç¡®ä¿æœ‰æ•ˆçš„èµ·å§‹æ¡ä»¶
            try_time = 0
            while r2gd < 0.3 and try_time < 100:  # ç¡®ä¿è·ç¦»ç›®æ ‡ä¸å¤ªè¿‘
                try_time = try_time + 1
                env.GenerateTargetPoint(mean_rate[length_index])
                o, r, d, goal_reach, r2gd, robot_pose = env.step()
                rate.sleep()
            
            try_time = 0
            while d and try_time < 100:  # ç¡®ä¿èµ·å§‹æ—¶æ— ç¢°æ’
                try_time = try_time + 1
                robot_size = env.ResetWorld(env_no, length1, length2, width)
                env.GenerateTargetPoint(mean_rate[length_index])
                o, r, d, goal_reach, r2gd, robot_pose = env.step()
                rate.sleep()

            try_time = 0
            while r2gd < 0.3 and try_time < 1000:
                try_time = try_time + 1
                env.GenerateTargetPoint(mean_rate[length_index])
                o, r, d, goal_reach, r2gd, robot_pose = env.step()
                rate.sleep()

            # è®¾ç½®episodeå‚æ•°
            max_ep_len = int(40 * 0.8**env_no / robot_size * 5)  # æ ¹æ®ç¯å¢ƒå’Œæœºå™¨äººå¤§å°è°ƒæ•´æœ€å¤§é•¿åº¦
            
            # ============= è®°å½•episodeå¼€å§‹ =============
            logger.log_episode_start(
                episode=episode,
                env_no=env_no,
                robot_size=[length1, length2, width],
                distance=r2gd,
                success_rate=mean_rate[length_index],
                max_steps=max_ep_len
            )

            # ============= è®¾ç½®stepä¸Šä¸‹æ–‡ä¸ºè®­ç»ƒ =============
            env.set_step_context("training")

            # Episodeæ‰§è¡Œ
            reset = False
            return_epoch = 0      # episodeæ€»å¥–åŠ±
            total_vel = 0         # æ€»é€Ÿåº¦
            ep_len = 0            # episodeé•¿åº¦
            d = False
            last_d = 0

            # ============= Episodeä¸»å¾ªç¯ =============
            while not reset:
                
                # åŠ¨ä½œé€‰æ‹©
                if episode > start_epoch:
                    a = get_action(o, deterministic=False)  # ä½¿ç”¨å­¦ä¹ çš„ç­–ç•¥
                else:
                    a = env.PIDController()  # é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨PIDæ§åˆ¶å™¨

                # æ‰§è¡ŒåŠ¨ä½œ
                env.Control(a)
                rate.sleep()
                env.Control(a)  # æ‰§è¡Œä¸¤æ¬¡ç¡®ä¿ç¨³å®š
                rate.sleep()
                
                past_a = a
                o2, r, d, goal_reach, r2gd, robot_pose2 = env.step()
                
                return_epoch = return_epoch + r
                total_vel = total_vel + a[0]

                # å­˜å‚¨ç»éªŒ
                replay_buffer.store(o, a, r, o2, d)
                ep_len += 1

                o = o2
                last_d = d

                # Episodeç»ˆæ­¢æ£€æŸ¥
                if d:
                    if episode > start_epoch:
                        # æ›´æ–°æˆåŠŸè®°å½•
                        suc_record[length_index, int(suc_pointer[length_index])] = goal_reach
                        
                        if env.stop_counter < 1.0:  # æ— ç¢°æ’
                            suc_record1[length_index, int(suc_pointer[length_index])] = goal_reach
                            suc_record2[length_index, int(suc_pointer[length_index])] = 0
                        else:  # æœ‰ç¢°æ’
                            suc_record1[length_index, int(suc_pointer[length_index])] = 0.0
                            suc_record2[length_index, int(suc_pointer[length_index])] = 0
                        
                        suc_pointer[length_index] = (suc_pointer[length_index] + 1) % 50
                    
                    # ============= è®°å½•episodeç»“æŸï¼ˆæ­£ç¡®åˆ¤æ–­çŠ¶æ€ï¼‰ =============
                    # å½“d=Trueæ—¶ï¼Œåªæœ‰ä¸¤ç§æƒ…å†µï¼šæˆåŠŸ(goal_reach=1)æˆ–ç¢°æ’(goal_reach=0)
                    # åœ¨æ­£å¼è®­ç»ƒé˜¶æ®µæ‰æ›´æ–°æ€»è®­ç»ƒæ­¥æ•°T
                    logger.log_episode_end(
                        episode=episode,
                        reward=return_epoch,
                        steps=ep_len,
                        success=bool(goal_reach),
                        crash=(not bool(goal_reach)),  # d=Trueä½†æ²¡æˆåŠŸï¼Œå°±æ˜¯ç¢°æ’
                        timeout=False,
                        update_training_steps=(episode > start_epoch)  # åªåœ¨æ­£å¼è®­ç»ƒæ—¶æ›´æ–°T
                    )
                    reset = True
                else:
                    if ep_len >= max_ep_len:  # è¶…æ—¶
                        if episode > start_epoch:
                            suc_record[length_index, int(suc_pointer[length_index])] = goal_reach
                            suc_record1[length_index, int(suc_pointer[length_index])] = goal_reach
                            suc_record2[length_index, int(suc_pointer[length_index])] = 1.0
                            suc_pointer[length_index] = (suc_pointer[length_index] + 1) % 50

                        # ============= è®°å½•episodeç»“æŸï¼ˆè¶…æ—¶ï¼‰ =============
                        logger.log_episode_end(
                            episode=episode,
                            reward=return_epoch,
                            steps=ep_len,
                            success=bool(goal_reach),
                            crash=False,
                            timeout=True,
                            update_training_steps=(episode > start_epoch)  # åªåœ¨æ­£å¼è®­ç»ƒæ—¶æ›´æ–°T
                        )
                        reset = True

                # ============= ç½‘ç»œè®­ç»ƒ =============
                if episode > start_epoch:
                    if goal_reach == 1 or env.crash_stop or (ep_len >= max_ep_len):
                        if ep_len == 0:
                            ep_len = 1
                        
                        reset = True
                        average_vel = total_vel / ep_len

                        # æ‰¹é‡è®­ç»ƒï¼šæ¯ä¸ªepisodeæ­¥æ•°å¯¹åº”ä¸€æ¬¡å‚æ•°æ›´æ–°
                        for j in range(ep_len):
                            T = T + 1
                            
                            # åŠ¨æ€ç»éªŒé‡‡æ ·ï¼šåå‘ä½¿ç”¨æ–°ç»éªŒ
                            start = np.minimum(
                                replay_buffer.size * (1.0 - (0.996 ** (j * 1.0 / ep_len * 1000.0))),
                                np.maximum(replay_buffer.size - 10000, 0),
                            )
                            
                            batch = replay_buffer.sample_batch(batch_size, start=start)
                            
                            # æ›´æ–°ç½‘ç»œ
                            pi_loss, q_loss = update_networks(batch)
                            
                            # è®°å½•åˆ°TensorBoard
                            if T % 100 == 0:
                                summary_writer.add_scalar('Loss/Policy', pi_loss, T)
                                summary_writer.add_scalar('Loss/Q_Network', q_loss, T)

                            # ============= å®šæœŸæ€§èƒ½æµ‹è¯• =============
                            if T % 10000 == 0:  # æ¯10000æ­¥æµ‹è¯•ä¸€æ¬¡
                                # ============= è®¾ç½®æµ‹è¯•é˜¶æ®µ =============
                                logger.set_phase("TESTING", test_round=T//10000)
                                logger.start_test_round(T//10000)
                                
                                # ============= è®¾ç½®stepä¸Šä¸‹æ–‡ä¸ºæµ‹è¯• =============
                                env.set_step_context("testing")
                                
                                # å­˜å‚¨æ¯ç»„çš„æµ‹è¯•ç»“æœ
                                group_results = {}
                                
                                for shape_no in range(5):  # æµ‹è¯•5ç§æœºå™¨äººå°ºå¯¸
                                    logger.log_test_group_start(shape_no, int(env_list[shape_no]))
                                    
                                    # å­˜å‚¨è¯¥ç»„çš„æ‰€æœ‰æµ‹è¯•ç»“æœ
                                    group_rewards = []
                                    group_success = []
                                    group_crashes = []
                                    
                                    for k in range(50):     # æ¯ç§å°ºå¯¸æµ‹è¯•50æ¬¡
                                        total_vel_test = 0
                                        return_epoch_test = 0
                                        ep_len_test = 0
                                        
                                        rospy.sleep(2.0)
                                        
                                        # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
                                        velcity = env.set_robot_pose_test(k, int(env_list[shape_no]), shape_no)
                                        env.GenerateTargetPoint_test(k, int(env_list[shape_no]), shape_no)
                                        max_ep_len = int(40 * 0.8 ** int(env_list[shape_no]) / velcity * 5)
                                        o, r, d, goal_reach, r2gd, robot_pose = env.step()

                                        # æµ‹è¯•episode
                                        for i in range(1000):
                                            a = get_action(o, deterministic=True)  # ç¡®å®šæ€§ç­–ç•¥æµ‹è¯•
                                            
                                            env.Control(a)
                                            rate.sleep()
                                            env.Control(a)
                                            rate.sleep()
                                            
                                            o2, r, d, goal_reach, r2gd, robot_pose2 = env.step()
                                            return_epoch_test = return_epoch_test + r
                                            total_vel_test = total_vel_test + a[0]
                                            ep_len_test += 1
                                            o = o2

                                            if d or (ep_len_test >= max_ep_len):
                                                # è®°å½•æµ‹è¯•ç»“æœ
                                                test_result_plot[hyper_exp, shape_no, k, test_time, 0] = return_epoch_test    # æ€»å¥–åŠ±
                                                test_result_plot[hyper_exp, shape_no, k, test_time, 1] = goal_reach           # æˆåŠŸæ ‡å¿—
                                                test_result_plot[hyper_exp, shape_no, k, test_time, 2] = (ep_len_test * 1.0 / max_ep_len)  # æ—¶é—´æ•ˆç‡
                                                test_result_plot[hyper_exp, shape_no, k, test_time, 3] = (1.0 * goal_reach - ep_len_test * 2.0 / max_ep_len)  # ç»¼åˆæŒ‡æ ‡
                                                test_result_plot[hyper_exp, shape_no, k, test_time, 4] = env.crash_stop       # ç¢°æ’æ ‡å¿—

                                                # æ”¶é›†ç»Ÿè®¡æ•°æ®
                                                group_rewards.append(return_epoch_test)
                                                group_success.append(goal_reach)
                                                group_crashes.append(env.crash_stop)
                                                
                                                break
                                    
                                    # è®¡ç®—è¯¥ç»„çš„ç»Ÿè®¡ç»“æœ
                                    group_result = {
                                        'success_rate': np.mean(group_success),
                                        'avg_reward': np.mean(group_rewards),
                                        'collision_rate': np.mean(group_crashes),
                                        'test_count': len(group_success)
                                    }
                                    group_results[shape_no] = group_result
                                    
                                    # è®°å½•è¯¥ç»„æµ‹è¯•å®Œæˆ
                                    logger.log_test_group_end(shape_no, group_result)
                                    
                                    # åŸæœ‰çš„æˆåŠŸç‡æ›´æ–°é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
                                    mean_rate[shape_no] = np.mean(
                                        test_result_plot[hyper_exp, shape_no, :, test_time, 1]
                                    )
                                    
                                    # è¯¾ç¨‹å­¦ä¹ ï¼šæˆåŠŸç‡è¾¾åˆ°90%æ—¶è§£é”ä¸‹ä¸€ç¯å¢ƒ
                                    if (mean_rate[shape_no] >= 0.90 and int(env_list[shape_no]) < 7):  # 8ä¸ªç¯å¢ƒ(0-7)
                                        env_list[shape_no] = env_list[shape_no] + 1
                                        succ_rate_test = 0
                                        mean_rate[shape_no] = 0.0
                                        suc_record[shape_no, :] = 0.0
                                        suc_pointer[shape_no] = 0.0
                                        logger.write_log(f"ğŸ‰å°ºå¯¸ç»„{shape_no}è§£é”ç¯å¢ƒ{int(env_list[shape_no])}")
                                    
                                    np.save(f'test_result_plot{sac_id}.npy', test_result_plot)

                                # ============= ç»“æŸæµ‹è¯•è½®æ¬¡ =============
                                logger.end_test_round(env_list)
                                test_time = test_time + 1
                                
                                # ============= ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ =============
                                logger.plot_learning_curves()
                                logger.plot_test_results_heatmap()
                                
                                # ============= ä¿å­˜æ•°æ® =============
                                logger.save_training_data()
                                
                                # ä¿å­˜æ¨¡å‹
                                if (test_time) % 1 == 0:
                                    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ä¿å­˜æ¨¡å‹
                                    model_dir = './saved_models'
                                    os.makedirs(model_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
                                    
                                    save_path = os.path.join(
                                        model_dir,
                                        f'configback540GMMdense{sac_id}lambda{hyper_exp}_{test_time}.pth'
                                    )
                                    
                                    # ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸
                                    torch.save({
                                        'main_model_state_dict': main_model.state_dict(),
                                        'target_model_state_dict': target_model.state_dict(),
                                        'pi_optimizer_state_dict': pi_optimizer.state_dict(),
                                        'q_optimizer_state_dict': q_optimizer.state_dict(),
                                        'test_time': test_time,
                                        'T': T,
                                        'episode': episode,
                                    }, save_path)
                                    
                                    logger.write_log(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
                                    rospy.sleep(3.0)
                                
                                # ============= é‡æ–°è®¾ç½®ä¸ºè®­ç»ƒé˜¶æ®µ =============
                                logger.set_phase("TRAINING")
                                env.set_step_context("training")
                                b_test = False

            episode = episode + 1
            epi_thr = epi_thr + 1

        # ============= å®éªŒç»“æŸæ—¶ç”Ÿæˆæ€»ç»“æŠ¥å‘Š =============
        logger.generate_summary_report()
        logger.write_log(f"âœ… å®éªŒ{hyper_exp}å®Œæˆ")

    # å…³é—­TensorBoard writer
    summary_writer.close()


def main():
    """ä¸»å‡½æ•°ï¼šå¯åŠ¨SACè®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹SACè®­ç»ƒ...")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    sac(actor_critic=core.MLPActorCritic, device=device)
    print("âœ… è®­ç»ƒå®Œæˆ!")


if __name__ == '__main__':
    # ============= ROSç¯å¢ƒåˆå§‹åŒ– =============
    random_number = random.randint(10000, 15000)
    port = str(random_number)
    os.environ["ROS_MASTER_URI"] = "http://localhost:" + port
    
    print(f"ğŸŒ å¯åŠ¨ROSç¯å¢ƒï¼Œç«¯å£: {port}")

    # å¯åŠ¨roscore
    subprocess.Popen(["roscore", "-p", port])
    time.sleep(2)
    print("âœ… Roscoreå¯åŠ¨æˆåŠŸ!")

    # å¯åŠ¨Stageä»¿çœŸå™¨
    world_file = "d8888153.world"
    subprocess.Popen(["rosrun", "stage_ros1", "stageros", world_file])
    print("âœ… Stageä»¿çœŸç¯å¢ƒå¯åŠ¨æˆåŠŸ!")
    time.sleep(2)
    
    main()


# ============= æ ¸å¿ƒå˜é‡è¯´æ˜ =============
"""
é‡è¦å˜é‡å«ä¹‰ï¼š

è®­ç»ƒç›¸å…³ï¼š
- episode: å½“å‰episodeç¼–å·
- T: æ€»è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨
- obs_dim: è§‚æµ‹ç»´åº¦(548 = 540æ¿€å…‰é›·è¾¾ + 8å…¶ä»–çŠ¶æ€)
- act_dim: åŠ¨ä½œç»´åº¦(2 = çº¿é€Ÿåº¦ + è§’é€Ÿåº¦)

è¯¾ç¨‹å­¦ä¹ ç›¸å…³ï¼š
- env_list[5]: 5ä¸ªæœºå™¨äººå°ºå¯¸ç»„å¯¹åº”çš„ç¯å¢ƒéš¾åº¦ç­‰çº§(0-6)
- mean_rate[5]: å„å°ºå¯¸ç»„çš„å¹³å‡æˆåŠŸç‡
- suc_record[5,50]: æˆåŠŸè®°å½•ï¼Œæ»‘åŠ¨çª—å£è®°å½•æœ€è¿‘50æ¬¡ç»“æœ
- new_env: å½“å‰è§£é”çš„æœ€é«˜ç¯å¢ƒç­‰çº§

æœºå™¨äººå‚æ•°ï¼š
- length1/length2: æœºå™¨äººå‰åé•¿åº¦
- width: æœºå™¨äººå®½åº¦
- length_index: æœºå™¨äººå°ºå¯¸ç»„ç´¢å¼•(0-4)

SACç®—æ³•å‚æ•°ï¼š
- gamma: æŠ˜æ‰£å› å­(0.99)
- polyak: ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç³»æ•°(0.995)
- alpha: ç†µæ­£åˆ™åŒ–ç³»æ•°(0.01)
- lr1/lr2: ä»·å€¼/ç­–ç•¥ç½‘ç»œå­¦ä¹ ç‡(1e-4)

æµ‹è¯•ç›¸å…³ï¼š
- test_time: æµ‹è¯•è½®æ¬¡è®¡æ•°å™¨
- test_result_plot: æµ‹è¯•ç»“æœå­˜å‚¨æ•°ç»„[å®éªŒæ•°, å°ºå¯¸ç»„, æµ‹è¯•æ¬¡æ•°, æµ‹è¯•è½®æ¬¡, æŒ‡æ ‡æ•°]

æ—¥å¿—ç³»ç»Ÿç›¸å…³ï¼š
- logger: TrainingLoggerå®ä¾‹ï¼Œç®¡ç†æ‰€æœ‰æ—¥å¿—å’Œå¯è§†åŒ–
- step_context: å½“å‰stepè°ƒç”¨çš„ä¸Šä¸‹æ–‡ï¼ˆinitialization/training/testingï¼‰

PyTorchç›¸å…³ï¼š
- device: è®¡ç®—è®¾å¤‡('cpu'æˆ–'cuda')
- main_model: ä¸»è®­ç»ƒç½‘ç»œ
- target_model: ç›®æ ‡ç½‘ç»œ
- pi_optimizer: ç­–ç•¥ç½‘ç»œä¼˜åŒ–å™¨
- q_optimizer: Qç½‘ç»œä¼˜åŒ–å™¨
- summary_writer: TensorBoardè®°å½•å™¨
"""